{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d1f85f-46ce-4b65-bf08-07e72bab66b5",
   "metadata": {},
   "source": [
    "# --- Day 15: Beacon Exclusion Zone ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6a8f61-912f-48ab-b142-acc333a4bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist(pos_a, pos_b):\n",
    "    # Calculate Manhattan distance\n",
    "    return abs(pos_b[0] - pos_a[0]) + abs(pos_b[1] - pos_a[1])\n",
    "\n",
    "def read_input(filename):\n",
    "    # Parse into list\n",
    "    # Each element of list corresponds to a sensor\n",
    "    # Each element is a triple [sensor, beacon, distance]\n",
    "    input_data = list()\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            sensor, beacon = line.strip().replace(\"Sensor at \", \"\").split(\": closest beacon is at \")\n",
    "            sensor_row = [tuple([int(x.split(\"=\")[1]) for x in y.split(\", \")]) for y in [sensor, beacon]]\n",
    "            sensor_row.append(calc_dist(sensor_row[1], sensor_row[0]))\n",
    "            input_data.append(sensor_row)\n",
    "                               \n",
    "    return input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018eee5c-c4aa-4e61-b939-3f860c7d0372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_min: -1789570\n",
      "x_max: 5315149\n",
      "y_min: -2035378\n",
      "y_max: 5428150\n",
      "beacon at: (-85806, 2000000) -> (1, 1703764)\n",
      "beacon at: (-85806, 2000000) -> (1, 1703764)\n",
      "beacon at: (-85806, 2000000) -> (1, 1703764)\n",
      "Row y=2000000 has 4811413 positions where a beacon cannot be present.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def pretty_print_matrix(mat):\n",
    "    for i in range(0, len(mat)):\n",
    "        print(\n",
    "            \"\".join(\n",
    "                [\n",
    "                    \".\" if y == 0 else \"S\" if y == -1 else \"B\" if y == -2 else \"#\"\n",
    "                    for y in mat[i]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    return 0\n",
    "\n",
    "def create_matrix(sensor_data, row_number, row_width, part1=True):\n",
    "    x_min = min(min(x[0][0] - x[2], x[1][0] - x[2]) for x in sensor_data)\n",
    "    x_max = max(max(x[0][0] + x[2], x[1][0] + x[2]) for x in sensor_data)\n",
    "    y_min = min(min(x[0][1] - x[2], x[1][1] - x[2]) for x in sensor_data)\n",
    "    y_max = max(max(x[0][1] + x[2], x[1][1] + x[2]) for x in sensor_data) \n",
    "    print(f\"x_min: {x_min}\")\n",
    "    print(f\"x_max: {x_max}\")\n",
    "    print(f\"y_min: {y_min}\")\n",
    "    print(f\"y_max: {y_max}\")\n",
    "    if part1:\n",
    "        m = np.full((1+(2*row_width), x_max-x_min), 0) \n",
    "    else:\n",
    "        m = np.full((1+(2*row_width), x_max-x_min), 0, dtype='uint8')\n",
    "    \n",
    "    #[ ] [ ] [ ] [ ]\n",
    "    #[ ] [ ] [ ] [ ]\n",
    "    # block by block\n",
    "    row_range = range((row_number-y_min)-row_width, (row_number-y_min)+row_width+1)\n",
    "    row_offset = row_number - row_width\n",
    "    #print(f\"row_range: {list(row_range)}\")\n",
    "    #pretty_print_matrix(m)\n",
    "    for r in sensor_data:\n",
    "        #print(r[0])\n",
    "        #print(r[1])\n",
    "        #print(r[1][1]-y_min)\n",
    "        if part1:\n",
    "            if r[0][1]-y_min in row_range:\n",
    "                print(f\"sensor at: {r[0]} -> ({r[0][1]-row_offset}, {r[0][0]-x_min})\")\n",
    "                m[r[0][1]-row_offset, r[0][0]-x_min] = -1\n",
    "\n",
    "            if r[1][1]-y_min in row_range:\n",
    "                print(f\"beacon at: {r[1]} -> ({r[1][1]-row_offset}, {r[1][0]-x_min})\")\n",
    "                m[r[1][1]-row_offset, r[1][0]-x_min] = -2\n",
    "            \n",
    "        indices = np.indices(m.shape)\n",
    "        y_indices = indices[0]\n",
    "        x_indices = indices[1]\n",
    "\n",
    "        locations = abs(x_indices - (r[0][0]-x_min)) + abs(y_indices - (r[0][1]-row_offset)) <= r[2]\n",
    "        m[locations & (m==0)]=1\n",
    "    \n",
    "    #pretty_print_matrix(m)\n",
    "    \n",
    "    print(f\"Row y={row_number} has {sum(m[row_width, :]==1)} positions where a beacon cannot be present.\")\n",
    "        \n",
    "        #pretty_print_matrix(m[range(row_number-1, row_number+2), :])\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "input_data = read_input(\"inputs/day15.txt\")\n",
    "create_matrix(input_data, 2000000, 1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10794a6-15cd-4265-8e78-4162759e7679",
   "metadata": {},
   "source": [
    "# Alternative approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c0163f6-9f58-45b9-b53f-f4268e7817ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting 1 chunks\n",
      "chunk_size: 64\n",
      "new chunk size: 32\n",
      "split into 4 chunks.\n",
      "splitting 4 chunks\n",
      "chunk_size: 32\n",
      "new chunk size: 16\n",
      "split into 16 chunks.\n"
     ]
    }
   ],
   "source": [
    "def sensor_more_top_left(sensor1, sensor2):\n",
    "    if (sensor1[0][0]+sensor1[0][1]) <= (sensor2[0][0]+sensor2[0][1]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def sort_sensors(sensor_data_to_sort):\n",
    "    i = 1\n",
    "    while i < len(sensor_data_to_sort):\n",
    "        j = i\n",
    "        while (\n",
    "            j > 0\n",
    "            and sensor_more_top_left(sensor_data_to_sort[j], sensor_data_to_sort[j-1])\n",
    "        ):\n",
    "            sensor_data_to_sort[j - 1], sensor_data_to_sort[j] = sensor_data_to_sort[j], sensor_data_to_sort[j - 1]\n",
    "            j -= 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return sensor_data_to_sort\n",
    "\n",
    "\n",
    "def split_chunk(chunks, search_size):\n",
    "    print(f\"splitting {len(chunks)} chunks\")\n",
    "    split_chunks = list()\n",
    "    for big_chunk in chunks:\n",
    "        chunk_size = (big_chunk[1][0]-big_chunk[0][0])+1\n",
    "\n",
    "        new_chunk_size = chunk_size//2\n",
    "        \n",
    "        if len(split_chunks) == 0:\n",
    "            print(f\"chunk_size: {chunk_size}\")            \n",
    "            print(f\"new chunk size: {new_chunk_size}\")\n",
    "        \n",
    "        #print(f\"big_chunk: {big_chunk}\")\n",
    "        #print(f\"big_chunk[0][0]: {big_chunk[0][0]}\")\n",
    "        new_chunks = list()\n",
    "        for i in range(0,2):\n",
    "            for j in range(0,2):\n",
    "                #print(f\"i={i},j={j}\")\n",
    "                x_min = big_chunk[0][0] + i*new_chunk_size\n",
    "                x_max = big_chunk[0][0] + (i+1)*(new_chunk_size) - 1\n",
    "                y_min = big_chunk[0][1] + j*new_chunk_size\n",
    "                y_max = big_chunk[0][1] + (j+1)*(new_chunk_size) - 1\n",
    "                chunk_corners = [(x_min, y_min), (x_max, y_min), (x_min, y_max), (x_max, y_max)]\n",
    "                #print(chunk_corners)\n",
    "                if (x_min <= search_size) or (y_min <= search_size):\n",
    "                    new_chunks.append(chunk_corners)\n",
    "        split_chunks += new_chunks\n",
    "    split_chunks = list(map(list,set(map(tuple,split_chunks))))\n",
    "    print(f\"split into {len(split_chunks)} chunks.\")\n",
    "\n",
    "    return split_chunks\n",
    "    \n",
    "import time \n",
    "def is_chunk_covered(sensor_data, chunkle, search_size):\n",
    "    \n",
    "    for x in range(chunkle[0][0], chunkle[3][0]+1):\n",
    "        for y in range(chunkle[0][1], chunkle[3][1]+1):\n",
    "            #print((x,y))\n",
    "            if all([calc_dist((x,y), sensor[0]) > sensor[2] for sensor in sensor_data]):\n",
    "                if (x >= 0) and (x <= search_size) and (y >= 0) and (y <= search_size):\n",
    "                    print(f\"x={x}, y={y}\")\n",
    "                    return False \n",
    "    \n",
    "    return True\n",
    "\n",
    "xmin = 10\n",
    "ymin = 10\n",
    "size = 63\n",
    "chunk = [(xmin, ymin), (xmin+size, ymin), (xmin, ymin+size), (xmin+size, ymin+size)]\n",
    "#print(chunk)\n",
    "split_chunk(split_chunk([chunk], 500), 500);\n",
    "#is_chunk_covered(input_data, chunk, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "baf57618-a283-4778-a099-1474767ed47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks: 31*31 = 961\n",
      "\n",
      "granularity: 0\n",
      "chunks: 961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 2507.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 330\n",
      "splitting 330 chunks\n",
      "chunk_size: 131072\n",
      "new chunk size: 65536\n",
      "split into 1320 chunks.\n",
      "\n",
      "granularity: 1\n",
      "chunks: 1320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 1184.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 203\n",
      "splitting 203 chunks\n",
      "chunk_size: 65536\n",
      "new chunk size: 32768\n",
      "split into 811 chunks.\n",
      "\n",
      "granularity: 2\n",
      "chunks: 811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 3251.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 182\n",
      "splitting 182 chunks\n",
      "chunk_size: 32768\n",
      "new chunk size: 16384\n",
      "split into 724 chunks.\n",
      "\n",
      "granularity: 3\n",
      "chunks: 724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 4212.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 219\n",
      "splitting 219 chunks\n",
      "chunk_size: 16384\n",
      "new chunk size: 8192\n",
      "split into 872 chunks.\n",
      "\n",
      "granularity: 4\n",
      "chunks: 872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 3091.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 277\n",
      "splitting 277 chunks\n",
      "chunk_size: 8192\n",
      "new chunk size: 4096\n",
      "split into 1104 chunks.\n",
      "\n",
      "granularity: 5\n",
      "chunks: 1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 1968.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 398\n",
      "splitting 398 chunks\n",
      "chunk_size: 4096\n",
      "new chunk size: 2048\n",
      "split into 1592 chunks.\n",
      "\n",
      "granularity: 6\n",
      "chunks: 1592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 1028.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 639\n",
      "splitting 639 chunks\n",
      "chunk_size: 2048\n",
      "new chunk size: 1024\n",
      "split into 2556 chunks.\n",
      "\n",
      "granularity: 7\n",
      "chunks: 2556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 445.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 1153\n",
      "splitting 1153 chunks\n",
      "chunk_size: 1024\n",
      "new chunk size: 512\n",
      "split into 4612 chunks.\n",
      "\n",
      "granularity: 8\n",
      "chunks: 4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 155.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 2218\n",
      "splitting 2218 chunks\n",
      "chunk_size: 512\n",
      "new chunk size: 256\n",
      "split into 8872 chunks.\n",
      "\n",
      "granularity: 9\n",
      "chunks: 8872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 47.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 4525\n",
      "splitting 4525 chunks\n",
      "chunk_size: 256\n",
      "new chunk size: 128\n",
      "split into 18100 chunks.\n",
      "\n",
      "granularity: 10\n",
      "chunks: 18100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 19/19 [00:01<00:00, 11.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 9218\n",
      "splitting 9218 chunks\n",
      "chunk_size: 128\n",
      "new chunk size: 64\n",
      "split into 36872 chunks.\n",
      "\n",
      "granularity: 11\n",
      "chunks: 36872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 19/19 [00:08<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 18270\n",
      "splitting 18270 chunks\n",
      "chunk_size: 64\n",
      "new chunk size: 32\n",
      "split into 73080 chunks.\n",
      "\n",
      "granularity: 12\n",
      "chunks: 73080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 19/19 [00:58<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 36459\n",
      "\n",
      "\n",
      "checking for coverage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████▊               | 28524/36459 [01:56<00:32, 246.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=3292963, y=3019123\n",
      "[(3292960, 3019104), (3292991, 3019104), (3292960, 3019135), (3292991, 3019135)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 36459/36459 [02:29<00:00, 243.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def part2(sensor_data, search_size, chunk_size, granularity_limit):\n",
    "    # split search space into chunks\n",
    "    sensor_data = sort_sensors(sensor_data)    \n",
    "    \n",
    "    # generate stack of chunks\n",
    "    # chunk is defined by its corners\n",
    "    chunks = []\n",
    "    \n",
    "    # add edge chunks\n",
    "    search_size\n",
    "    \n",
    "    # number of chunks along an edge\n",
    "    num_chunks_side = (search_size+chunk_size)//chunk_size\n",
    "        \n",
    "    print(f\"number of chunks: {num_chunks_side}*{num_chunks_side} = {num_chunks_side*num_chunks_side}\")\n",
    "    # horizontal\n",
    "    for i in range(0, num_chunks_side):\n",
    "        # vertical\n",
    "        for j in range(0, num_chunks_side):\n",
    "            x_min = i*chunk_size\n",
    "            x_max = (i+1)*(chunk_size) - 1\n",
    "            y_min = j*chunk_size\n",
    "            y_max = (j+1)*(chunk_size) - 1\n",
    "            chunk_corners = [(x_min, y_min), (x_max, y_min), (x_min, y_max), (x_max, y_max)]\n",
    "            chunks.append(chunk_corners)\n",
    "                \n",
    "    # 'divide and conquer approach' - \n",
    "    # first remove chunks that are fully in sensor's range\n",
    "    # for sensor:\n",
    "    #     if chunk in sensor range:\n",
    "    #         delete chunk\n",
    "    granularity = 0\n",
    "    while granularity < granularity_limit:    \n",
    "        print(f\"\\ngranularity: {granularity}\")\n",
    "        print(f\"chunks: {len(chunks)}\")                    \n",
    "        for sensor in tqdm(sensor_data):\n",
    "            #print(f\"\\nsensor: {sensor}\")\n",
    "            for chunk in chunks:\n",
    "                if (calc_dist(chunk[0], sensor[0]) > sensor[2]) or \\\n",
    "                    (calc_dist(chunk[1], sensor[0]) > sensor[2]) or \\\n",
    "                    (calc_dist(chunk[2], sensor[0]) > sensor[2]) or \\\n",
    "                    (calc_dist(chunk[3], sensor[0]) > sensor[2]):\n",
    "                    pass\n",
    "                else:\n",
    "                    chunks.remove(chunk)\n",
    "                    #print(f\"deleted chunk {chunk}. {len(chunks)} remain\")\n",
    "        print(f\"chunks remaining: {len(chunks)}\")\n",
    "        granularity += 1\n",
    "        if granularity < granularity_limit:\n",
    "            chunks = split_chunk(chunks, search_size)\n",
    "    \n",
    "    print(f\"\\n\\nchecking for coverage\")\n",
    "\n",
    "    for chunk in tqdm(chunks):\n",
    "        #print(chunk)        \n",
    "        if not is_chunk_covered(sensor_data, chunk, search_size):\n",
    "            print(chunk) \n",
    "    return 0\n",
    "     \n",
    "\n",
    "input_data = read_input(\"inputs/day15.txt\")\n",
    "part2(input_data, 4000000, 2**17, 13);\n",
    "#part2(input_data, 20, 2**4, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117e8678-73dd-4e11-b923-15b3d736d240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13171855019123"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3292963*4000000) + 3019123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46649ce4-4e70-42dc-a838-a03a00b435f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
