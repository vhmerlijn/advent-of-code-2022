{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96d1f85f-46ce-4b65-bf08-07e72bab66b5",
   "metadata": {},
   "source": [
    "# --- Day 15: Beacon Exclusion Zone ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6a8f61-912f-48ab-b142-acc333a4bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist(pos_a, pos_b):\n",
    "    # Calculate Manhattan distance\n",
    "    return abs(pos_b[0] - pos_a[0]) + abs(pos_b[1] - pos_a[1])\n",
    "\n",
    "def read_input(filename):\n",
    "    # Parse into list\n",
    "    # Each element of list corresponds to a sensor\n",
    "    # Each element is a triple [sensor, beacon, distance]\n",
    "    input_data = list()\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            sensor, beacon = line.strip().replace(\"Sensor at \", \"\").split(\": closest beacon is at \")\n",
    "            sensor_row = [tuple([int(x.split(\"=\")[1]) for x in y.split(\", \")]) for y in [sensor, beacon]]\n",
    "            sensor_row.append(calc_dist(sensor_row[1], sensor_row[0]))\n",
    "            input_data.append(sensor_row)\n",
    "                               \n",
    "    return input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018eee5c-c4aa-4e61-b939-3f860c7d0372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_min: -1789570\n",
      "x_max: 5315149\n",
      "y_min: -2035378\n",
      "y_max: 5428150\n",
      "beacon at: (-85806, 2000000) -> (1, 1703764)\n",
      "beacon at: (-85806, 2000000) -> (1, 1703764)\n",
      "beacon at: (-85806, 2000000) -> (1, 1703764)\n",
      "Row y=2000000 has 4811413 positions where a beacon cannot be present.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def pretty_print_matrix(mat):\n",
    "    for i in range(0, len(mat)):\n",
    "        print(\n",
    "            \"\".join(\n",
    "                [\n",
    "                    \".\" if y == 0 else \"S\" if y == -1 else \"B\" if y == -2 else \"#\"\n",
    "                    for y in mat[i]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    return 0\n",
    "\n",
    "def create_matrix(sensor_data, row_number, row_width, part1=True):\n",
    "    x_min = min(min(x[0][0] - x[2], x[1][0] - x[2]) for x in sensor_data)\n",
    "    x_max = max(max(x[0][0] + x[2], x[1][0] + x[2]) for x in sensor_data)\n",
    "    y_min = min(min(x[0][1] - x[2], x[1][1] - x[2]) for x in sensor_data)\n",
    "    y_max = max(max(x[0][1] + x[2], x[1][1] + x[2]) for x in sensor_data) \n",
    "    print(f\"x_min: {x_min}\")\n",
    "    print(f\"x_max: {x_max}\")\n",
    "    print(f\"y_min: {y_min}\")\n",
    "    print(f\"y_max: {y_max}\")\n",
    "    if part1:\n",
    "        m = np.full((1+(2*row_width), x_max-x_min), 0) \n",
    "    else:\n",
    "        m = np.full((1+(2*row_width), x_max-x_min), 0, dtype='uint8')\n",
    "    \n",
    "    #[ ] [ ] [ ] [ ]\n",
    "    #[ ] [ ] [ ] [ ]\n",
    "    # block by block\n",
    "    row_range = range((row_number-y_min)-row_width, (row_number-y_min)+row_width+1)\n",
    "    row_offset = row_number - row_width\n",
    "    #print(f\"row_range: {list(row_range)}\")\n",
    "    #pretty_print_matrix(m)\n",
    "    for r in sensor_data:\n",
    "        #print(r[0])\n",
    "        #print(r[1])\n",
    "        #print(r[1][1]-y_min)\n",
    "        if part1:\n",
    "            if r[0][1]-y_min in row_range:\n",
    "                print(f\"sensor at: {r[0]} -> ({r[0][1]-row_offset}, {r[0][0]-x_min})\")\n",
    "                m[r[0][1]-row_offset, r[0][0]-x_min] = -1\n",
    "\n",
    "            if r[1][1]-y_min in row_range:\n",
    "                print(f\"beacon at: {r[1]} -> ({r[1][1]-row_offset}, {r[1][0]-x_min})\")\n",
    "                m[r[1][1]-row_offset, r[1][0]-x_min] = -2\n",
    "            \n",
    "        indices = np.indices(m.shape)\n",
    "        y_indices = indices[0]\n",
    "        x_indices = indices[1]\n",
    "\n",
    "        locations = abs(x_indices - (r[0][0]-x_min)) + abs(y_indices - (r[0][1]-row_offset)) <= r[2]\n",
    "        m[locations & (m==0)]=1\n",
    "    \n",
    "    #pretty_print_matrix(m)\n",
    "    \n",
    "    print(f\"Row y={row_number} has {sum(m[row_width, :]==1)} positions where a beacon cannot be present.\")\n",
    "        \n",
    "        #pretty_print_matrix(m[range(row_number-1, row_number+2), :])\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "input_data = read_input(\"inputs/day15.txt\")\n",
    "create_matrix(input_data, 2000000, 1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10794a6-15cd-4265-8e78-4162759e7679",
   "metadata": {},
   "source": [
    "# Alternative approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c0163f6-9f58-45b9-b53f-f4268e7817ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_more_top_left(sensor1, sensor2):\n",
    "    if (sensor1[0][0]+sensor1[0][1]) <= (sensor2[0][0]+sensor2[0][1]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def sort_sensors(sensor_data_to_sort):\n",
    "    i = 1\n",
    "    while i < len(sensor_data_to_sort):\n",
    "        j = i\n",
    "        while (\n",
    "            j > 0\n",
    "            and sensor_more_top_left(sensor_data_to_sort[j], sensor_data_to_sort[j-1])\n",
    "        ):\n",
    "            sensor_data_to_sort[j - 1], sensor_data_to_sort[j] = sensor_data_to_sort[j], sensor_data_to_sort[j - 1]\n",
    "            j -= 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return sensor_data_to_sort\n",
    "\n",
    "\n",
    "def split_chunk(chunks, search_size):\n",
    "    print(f\"splitting {len(chunks)} chunks\")\n",
    "    split_chunks = list()\n",
    "    for big_chunk in chunks:\n",
    "        chunk_size = (big_chunk[1][0]-big_chunk[0][0])+1\n",
    "\n",
    "        new_chunk_size = chunk_size//2\n",
    "        \n",
    "        if len(split_chunks) == 0:\n",
    "            print(f\"new chunk size: {new_chunk_size}\")\n",
    "            \n",
    "        new_chunks = list()\n",
    "        for i in range(0,2):\n",
    "            for j in range(0,2):\n",
    "                x_min = big_chunk[0][0] + i*new_chunk_size\n",
    "                x_max = big_chunk[0][0] + (i+1)*(new_chunk_size) - 1\n",
    "                y_min = big_chunk[0][0] + j*new_chunk_size\n",
    "                y_max = big_chunk[0][0] + (j+1)*(new_chunk_size) - 1\n",
    "                chunk_corners = [(x_min, y_min), (x_max, y_min), (x_min, y_max), (x_max, y_max)]\n",
    "                if (x_min <= search_size) or (y_min <= search_size):\n",
    "                    new_chunks.append(chunk_corners)\n",
    "        split_chunks += new_chunks\n",
    "    print(f\"split into {len(split_chunks)} chunks.\")\n",
    "\n",
    "    return split_chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf57618-a283-4778-a099-1474767ed47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks: 31*31 = 961\n",
      "\n",
      "granularity: 0\n",
      "chunks: 961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 1148.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 330\n",
      "splitting 330 chunks\n",
      "new chunk size: 65536\n",
      "split into 1320 chunks.\n",
      "\n",
      "granularity: 1\n",
      "chunks: 1320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 1027.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 607\n",
      "splitting 607 chunks\n",
      "new chunk size: 32768\n",
      "split into 2415 chunks.\n",
      "\n",
      "granularity: 2\n",
      "chunks: 2415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 297.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 1093\n",
      "splitting 1093 chunks\n",
      "new chunk size: 16384\n",
      "split into 4307 chunks.\n",
      "\n",
      "granularity: 3\n",
      "chunks: 4307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 112.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 2003\n",
      "splitting 2003 chunks\n",
      "new chunk size: 8192\n",
      "split into 7975 chunks.\n",
      "\n",
      "granularity: 4\n",
      "chunks: 7975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 19/19 [00:00<00:00, 31.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 3834\n",
      "splitting 3834 chunks\n",
      "new chunk size: 4096\n",
      "split into 15295 chunks.\n",
      "\n",
      "granularity: 5\n",
      "chunks: 15295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 7446\n",
      "splitting 7446 chunks\n",
      "new chunk size: 2048\n",
      "split into 29756 chunks.\n",
      "\n",
      "granularity: 6\n",
      "chunks: 29756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 19/19 [00:16<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 14664\n",
      "splitting 14664 chunks\n",
      "new chunk size: 1024\n",
      "split into 58643 chunks.\n",
      "\n",
      "granularity: 7\n",
      "chunks: 58643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 19/19 [01:04<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 29109\n",
      "splitting 29109 chunks\n",
      "new chunk size: 512\n",
      "split into 116399 chunks.\n",
      "\n",
      "granularity: 8\n",
      "chunks: 116399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 19/19 [04:29<00:00, 14.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks remaining: 57989\n",
      "splitting 57989 chunks\n",
      "new chunk size: 256\n",
      "split into 231932 chunks.\n",
      "\n",
      "granularity: 9\n",
      "chunks: 231932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████▊                                                    | 3/19 [07:10<38:14, 143.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     60\u001b[0m input_data \u001b[38;5;241m=\u001b[39m read_input(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs/day15.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mpart2\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mpart2\u001b[0;34m(sensor_data, search_size, chunk_size, granularity_limit)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m             \u001b[43mchunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m             \u001b[38;5;66;03m#print(f\"deleted chunk {chunk}. {len(chunks)} remain\")\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks remaining: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def part2(sensor_data, search_size, chunk_size, granularity_limit):\n",
    "    # split search space into chunks\n",
    "    sensor_data = sort_sensors(sensor_data)    \n",
    "    \n",
    "    # generate stack of chunks\n",
    "    # chunk is defined by its corners\n",
    "    chunks = []\n",
    "    \n",
    "    # add edge chunks\n",
    "    search_size\n",
    "    \n",
    "    # number of chunks along an edge\n",
    "    num_chunks_side = (search_size+chunk_size)//chunk_size\n",
    "        \n",
    "    print(f\"number of chunks: {num_chunks_side}*{num_chunks_side} = {num_chunks_side*num_chunks_side}\")\n",
    "    # horizontal\n",
    "    for i in range(0, num_chunks_side):\n",
    "        # vertical\n",
    "        for j in range(0, num_chunks_side):\n",
    "            x_min = i*chunk_size\n",
    "            x_max = (i+1)*(chunk_size) - 1\n",
    "            y_min = j*chunk_size\n",
    "            y_max = (j+1)*(chunk_size) - 1\n",
    "            chunk_corners = [(x_min, y_min), (x_max, y_min), (x_min, y_max), (x_max, y_max)]\n",
    "            chunks.append(chunk_corners)\n",
    "                \n",
    "    # 'divide and conquer approach' - \n",
    "    # first remove chunks that are fully in sensor's range\n",
    "    # for sensor:\n",
    "    #     if chunk in sensor range:\n",
    "    #         delete chunk\n",
    "    granularity = 0\n",
    "    while granularity < granularity_limit:    \n",
    "        print(f\"\\ngranularity: {granularity}\")\n",
    "        print(f\"chunks: {len(chunks)}\")                    \n",
    "        for sensor in tqdm(sensor_data):\n",
    "            #print(f\"\\nsensor: {sensor}\")\n",
    "            for chunk in chunks:\n",
    "                if (calc_dist(chunk[0], sensor[0]) > sensor[2]) or \\\n",
    "                    (calc_dist(chunk[1], sensor[0]) > sensor[2]) or \\\n",
    "                    (calc_dist(chunk[2], sensor[0]) > sensor[2]) or \\\n",
    "                    (calc_dist(chunk[3], sensor[0]) > sensor[2]):\n",
    "                    pass\n",
    "                else:\n",
    "                    chunks.remove(chunk)\n",
    "                    #print(f\"deleted chunk {chunk}. {len(chunks)} remain\")\n",
    "        print(f\"chunks remaining: {len(chunks)}\")\n",
    "        granularity += 1\n",
    "        if granularity < granularity_limit:\n",
    "            chunks = split_chunk(chunks, search_size)\n",
    "    print(f\"\\n\\nchecking for coverage\")\n",
    "\n",
    "    for chunk in tqdm(chunks):\n",
    "        #print(chunk)        \n",
    "        if not is_chunk_covered(sensor_data, chunk):\n",
    "            print(chunk) \n",
    "    return 0\n",
    "     \n",
    "\n",
    "input_data = read_input(\"inputs/day15.txt\")\n",
    "part2(input_data, 4000000, 2**17, 11);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d17fa9d-01ed-4022-8707-9263f050ded0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "def is_chunk_covered(sensor_data, chunkle):\n",
    "    \n",
    "    #tic = time.perf_counter()\n",
    "\n",
    "    for x in range(chunkle[0][0], chunkle[3][0]+1):\n",
    "        for y in range(chunkle[0][1], chunkle[3][1]+1):\n",
    "            #print((x,y))\n",
    "            if all([calc_dist((x,y), sensor[0]) > sensor[2] for sensor in sensor_data]):\n",
    "                return False\n",
    "    \n",
    "#     toc = time.perf_counter()\n",
    "#     print(f\"For loops: {toc - tic:0.4f}s\")    \n",
    "    \n",
    "#     def func(x, y):\n",
    "#         return not all([calc_dist((x,y), sensor[0]) > sensor[2] for sensor in sensor_data])\n",
    "    \n",
    "#     vfunc = np.vectorize(func)\n",
    "    \n",
    "#     tic = time.perf_counter()    \n",
    "#     mgx, mgy = np.meshgrid(range(chunkle[0][1], chunkle[3][1]+1), range(chunkle[0][0], chunkle[3][0]+1))\n",
    "#     print(f\"meshgrid: {np.all(vfunc(mgx, mgy))}\")\n",
    "#     #print(mgx)\n",
    "#     toc = time.perf_counter()\n",
    "#     print(f\"meshgrid: {toc - tic:0.4f}s\")    \n",
    "    \n",
    "    return True\n",
    "\n",
    "xmin = 10\n",
    "ymin = 10\n",
    "size = 100\n",
    "chunk = [(xmin, ymin), (xmin+size, ymin), (xmin, ymin+size), (xmin+size, ymin+size)]\n",
    "is_chunk_covered(input_data, chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32a4bd-28c8-4115-b229-005ecc7dae59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "venv39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
